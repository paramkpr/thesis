Machine learning has fundamentally transformed decision-making by allowing algorithms to discover intricate and
meaningful patterns directly from data, without relying explicitly on predefined rules. Rather than encoding
human expertise through manual programming, machine learning algorithms generalize from examples. This inductive
process, which generalizes observed cases to unseen scenarios, enables the algorithm to identify underlying patterns
from historical examples and predict future outcomes. However, such a reliance on historical data inherently carries
    risks, especially when data reflects existing societal biases, stereotypes, or inequalities.

\subsection{Algorithmic Bias}\label{subsec:algorithmic_bias}

Algorithmic bias refers to systematic and repeatable errors or unfair outcomes produced by machine learning models
due to biases embedded within the training data or algorithm design. According to existing literature
\cite{barocas2016big,pessach2020algorithmic}, algorithmic bias commonly originates from:

\begin{itemize}
    \item \textbf{Biases inherent in training datasets:} These biases result from biased human decisions, measurement errors,
     reporting inaccuracies, or historical prejudices embedded in datasets. Machine learning algorithms, aiming at optimizing
     prediction accuracy, often replicate these biases.
    \item \textbf{Biases due to missing data:} When datasets lack sufficient representation from certain groups or have
     significant data omissions, the resulting models fail to accurately represent the target population.
    \item \textbf{Algorithmic optimization bias:} Typical optimization objectives, such as minimizing aggregate prediction
     errors, tend to favor majority groups, often leading to poorer performance for minority groups.
    \item \textbf{Bias from proxy attributes:} Non-sensitive attributes may indirectly capture sensitive information
     (e.g., race, gender, age), unintentionally introducing biases even when sensitive attributes are explicitly excluded
     from the dataset.
\end{itemize}

\subsection{Defining Fairness in Machine Learning}\label{subsec:fairness_definitions}

Given the increasing use of machine learning in high-stakes domains, rigorous fairness definitions have emerged to guide
 algorithmic development. These definitions typically fall into two broad categories: individual fairness and group fairness.

\paragraph{Individual Fairness.}  
Individual fairness requires models to produce similar outputs for similar individuals, where similarity is assessed based
on relevant non-sensitive features. Formally, individual fairness can be articulated using Lipschitz continuity
as follows \cite{dwork2012fairness}:
\[
d(\text{output}(x), \text{output}(x')) \leq \rho \cdot d(x, x'),
\]
where \(x\) and \(x'\) are individuals with comparable non-sensitive attributes, and \(\rho\) is a small constant. This definition
 emphasizes the fair treatment of similar cases on an individual basis, ensuring minimal unjustified variability.

\paragraph{Group Fairness.}  
Group fairness demands that statistical outcomes of algorithms be equitable across predefined demographic groups.
 This approach explicitly acknowledges and attempts to rectify societal disparities. Notable metrics
  for group fairness include:

\begin{itemize}
    \item \textbf{Demographic Parity:} Ensures equal rates of positive predictions across demographic groups:
    \[
    P(R = 1 \mid A = a) = P(R = 1 \mid A = b),
    \]
    where \(R\) denotes the model's prediction and \(A\) represents a protected attribute (e.g., race, gender).

    \item \textbf{Equal Opportunity:} Requires equal true positive rates among groups, ensuring fairness in the
     allocation of positive outcomes given actual positives:
    \[
    P(R = 1 \mid Y = 1, A = a) = P(R = 1 \mid Y = 1, A = b),
    \]
    with \(Y\) representing the true outcome.

    \item \textbf{Equalized Odds:} Further requires equal true positive and false positive rates across groups,
     encompassing both success and error equity:
    \[
    P(R = 1 \mid Y = y, A = a) = P(R = 1 \mid Y = y, A = b), \quad \forall y \in \{0, 1\}.
    \]
\end{itemize}

Each of these metrics presents trade-offs, and no universal criterion exists to satisfy all simultaneously,
 leading to a fundamental tension explored later.

 
 \subsection{Real-World Instances and Ethical Dimensions of Algorithmic Bias}\label{subsec:real_world_bias}

 Concrete examples vividly illustrate the risks associated with biased algorithms. One widely cited example is the COMPAS algorithm, frequently used in criminal justice for predicting recidivism. Investigations revealed significant racial biases, incorrectly labeling Black defendants as high-risk at nearly twice the rate of White defendants who later re-offended~\cite{angwin2016machine}. Similarly, facial recognition software has consistently demonstrated higher error rates for darker-skinned individuals, exacerbating risks of racial profiling and wrongful identification.
 
 Algorithmic biases also permeate employment contexts, where historical data reflecting past hiring decisions embed biases against women or minority groups, perpetuating discrimination through ostensibly neutral automated decision-making systems. When training algorithms on historical employment records, biases and stereotypes embedded in the data disproportionately disadvantage female candidates.
 
 A particularly telling example emerges from machine translation. Consider translating sentences from English to Turkish and then back into English, as illustrated in Figures~\ref{fig:eng-to-turkish} and~\ref{fig:turkish-to-eng}. Turkish pronouns are gender-neutral, but when translated back into English, gender-specific pronouns are inferred based on statistical associations. As a result, occupations stereotypically associated with men—such as ``engineer'' or ``doctor''—are translated back using male pronouns, while occupations stereotypically associated with women—such as ``nurse''—return female pronouns. This phenomenon arises from two biases embedded in training datasets: real-world labor market statistics reflecting historical occupational distributions and the ``male-as-norm'' bias, whereby male pronouns are preferentially selected when gender is ambiguous or unknown~\cite{caliskan2017semantics}.
 
 \begin{figure}[h]
     \centering
     \includegraphics[width=0.9\textwidth]{sections/background/eng-to-turkish.png}
     \caption{Translations of gender-specific English sentences into gender-neutral Turkish.}
     \label{fig:eng-to-turkish}
 \end{figure}
 
 \begin{figure}[h]
     \centering
     \includegraphics[width=0.9\textwidth]{sections/background/turkish-to-eng.png}
     \caption{Translations from gender-neutral Turkish sentences back into English reveal embedded gender biases.}
     \label{fig:turkish-to-eng}
 \end{figure}
 
 Attempts to mitigate biases by removing explicitly sensitive attributes (such as gender or race) from training datasets frequently fall short due to \textit{proxy variables}. Proxy attributes—such as the age at which individuals start programming—can inadvertently encode sensitive information like gender, reinforcing biases even in their absence. Additionally, biases due to disparities in sample sizes among demographic groups lead to poorer model performance for minority groups, reinforcing systematic inequities~\cite{barocas2016big}.
 
 Beyond technical considerations, fairness intersects profoundly with ethical and legal imperatives. Legal frameworks such as Title VII of the U.S. Civil Rights Act mandate nondiscrimination in employment. Similarly, regulatory initiatives like the European Union’s GDPR and proposed AI Act embed fairness, transparency, and equity into regulatory requirements for algorithmic systems.
 
 Ethically, ensuring fairness aligns closely with broader principles of justice and equity, particularly as algorithmic systems increasingly influence societal outcomes such as employment, education, and criminal justice. Effective fairness interventions help prevent reinforcing historical injustices, foster social equity, and maintain public trust in algorithm-driven decision-making.
 
 \subsection{Challenges, Limitations, and Future Directions}\label{subsec:challenges_future}
 
 While fairness research in machine learning represents a critical endeavor, it faces inherent limitations and challenges. Foremost is the tension among different fairness criteria, as previously discussed (see Section~\ref{subsec:fairness_definitions}). It is mathematically impossible to simultaneously satisfy all fairness notions, such as Demographic Parity, Equal Opportunity, and Predictive Parity. Consequently, context-specific, nuanced solutions must be adopted, underscoring the difficulty of universal fairness guarantees.
 
 Furthermore, fairness interventions typically target algorithmic aspects of decision-making. Yet, algorithmic bias is deeply rooted in broader societal, historical, cultural, and economic contexts. Simply addressing biases within algorithms—without tackling underlying structural inequalities—may prove insufficient. For example, correcting gender bias in employment algorithms does not address the deeper systemic issue of unequal access to educational opportunities or societal expectations that shape labor distributions.
 
 Addressing these multidimensional challenges requires extensive cross-disciplinary collaboration involving researchers from sociology, law, ethics, economics, cultural studies, and computer science. Developing truly equitable systems necessitates a holistic approach, acknowledging that algorithmic fairness is merely one aspect of a much larger societal problem. Thus, future research should increasingly focus on integrated methodologies that combine technical algorithmic solutions with broader structural, legal, cultural, and ethical insights to effectively mitigate algorithmic bias and its societal consequences.
 